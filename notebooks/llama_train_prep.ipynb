{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:10<00:00,  3.16it/s]\n"
     ]
    }
   ],
   "source": [
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "device_map = \"auto\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(prompt):\n",
    "    # print(prompt)\n",
    "    prompt = prompt[\"prompt\"]\n",
    "\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "    )\n",
    "    user_prompt = prompt[: prompt.index(\"### Assistant:\")]\n",
    "    tokenized_user_prompt = tokenizer(\n",
    "        user_prompt,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "    tokenized_prompt[\"labels\"] = [-100] * (prompt_len) + tokenized_prompt[\"input_ids\"][\n",
    "        prompt_len:\n",
    "    ]\n",
    "    return tokenized_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/kosenko/.cache/huggingface/datasets/json/default-8fca4c8833304c51/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 477.06it/s]\n",
      "                                                                                  \r"
     ]
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"/home/kosenko/ru_chatGPT/ru_instruct/ru_instruct/sandbox/datasets_processed/IlyaGusev_ru_turbo_alpaca_formatted.json\"\n",
    "}\n",
    "data = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=data_files,\n",
    ")\n",
    "\n",
    "# всеравно мы никак не можем адекватно мерить модель\n",
    "# тогда зачем тратить на валидацию данные?\n",
    "train_data = (\n",
    "    data[\"train\"]\n",
    "    .shuffle()\n",
    "    .map(\n",
    "        generate_and_tokenize_prompt,\n",
    "        num_proc=32,\n",
    "        # batched=True,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Как вы относитесь к конкуренции на рабочем месте?\n",
      "\n",
      "### Response:\n",
      "### Assistant:\n",
      "Я считаю, что конкуренция на рабочем месте может быть полезной, если она справедливая и здоровая. Конкуренция может стимулировать улучшение работы и повышение производительности. Однако, я также считаю, что важно сохранять здоровую рабочую среду и отношения с коллегами. Я всегда стараюсь помогать своим коллегам и делиться своими знаниями и опытом, чтобы мы могли достигать успеха вместе.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][1000][\"prompt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 75368\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Я люблю гулять в парке.\n",
      "Переведи следующее высказывание на немецкий язык.\n",
      "### Response:\n",
      "### Assistant:\n",
      "Eingabe: Ich liebe es, im Park spazieren zu gehen.\n",
      "\n",
      "\n",
      "<unk>\n",
      "\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
      "\n",
      "### Human: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Я люблю гулять в парке.\n",
      "Переведи следующее высказывание на немецкий язык.\n",
      "### Response:\n",
      "### Assistant:\n",
      "Eingabe: Ich liebe es, im Park spazieren zu gehen.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][\"prompt\"])\n",
    "print(tokenizer.decode(train_data[0][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.remove_columns(\"prompt\")\n",
    "splited_dataset = train_data.train_test_split(train_size=0.98, test_size=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 73860\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "\ttokenizer=tokenizer,\n",
    "\tpad_to_multiple_of=8,\n",
    " \treturn_tensors=\"pt\", \n",
    "  \tpadding=True\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "\tdataset=splited_dataset['train'],\n",
    "\tbatch_size=16,\n",
    "\t# num_workers=self.hparams.num_workers,\n",
    "\tpin_memory=True,\n",
    "\tshuffle=True,\n",
    " \tcollate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,  ..., 29973,    13,    13],\n",
       "        [    0,     0,     0,  ..., 29973,    13,    13],\n",
       "        [    0,     0,     0,  ..., 29991,    13,    13],\n",
       "        ...,\n",
       "        [    0,     0,     0,  ...,  7337,    13,    13],\n",
       "        [    0,     0,     0,  ..., 29889,    13,    13],\n",
       "        [    0,     0,     0,  ..., 29889,    13,    13]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  ..., 29973,    13,    13],\n",
       "        [ -100,  -100,  -100,  ..., 29973,    13,    13],\n",
       "        [ -100,  -100,  -100,  ..., 29991,    13,    13],\n",
       "        ...,\n",
       "        [ -100,  -100,  -100,  ...,  7337,    13,    13],\n",
       "        [ -100,  -100,  -100,  ..., 29889,    13,    13],\n",
       "        [ -100,  -100,  -100,  ..., 29889,    13,    13]])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
